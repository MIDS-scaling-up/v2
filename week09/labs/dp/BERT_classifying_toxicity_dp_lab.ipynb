{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will continue on the [Conversation AI](https://conversationai.github.io/) dataset seen in [week 6 homework and lab](https://github.com/MIDS-scaling-up/v2/tree/master/week06). \n",
    " \n",
    "The original notebook ran on a single GPU. The goal of this lab is to evaluate different options of making it run on more than one GPU\n",
    "  \n",
    "*Disclaimer: the dataset used contains text that may be considered profane, vulgar, or offensive.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and prepare the datasets\n",
    "# this only needs to be done once.. \n",
    "# notice where the data goes\n",
    "!mkdir data\n",
    "# Download the training and the test corpus\n",
    "!wget -nv --show-progress -O data/test.csv.zip https://www.dropbox.com/s/xp6bo8yo1vbv5yg/test.csv.zip?dl=1\n",
    "!wget -nv --show-progress -O data/train.csv.zip https://www.dropbox.com/s/xei6z41mfrcnxcd/train.csv.zip?dl=1\n",
    "# Download the pretrained weights for bert base. \n",
    "!wget -nv --show-progress -O data/uncased_L-12_H-768_A-12.zip \\\n",
    "        https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!wget -nv --show-progress  -O data/cased_L-12_H-768_A-12.zip \\\n",
    "        https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
    "# unzip weights & conifg and remove the original zip\n",
    "!unzip -d data/ data/cased_L-12_H-768_A-12.zip && rm data/cased_L-12_H-768_A-12.zip\n",
    "!unzip -d data/ data/uncased_L-12_H-768_A-12.zip && rm data/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# from tqdm import tqdm, tqdm_notebook\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's activate CUDA for GPU based operations\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the PATH variable to whereever your `week06/hw` directory is located.  \n",
    "**For the final run we would like you to have a train_size of at least 1 Million rows, and a valid size of at least 500K rows. When you first run the script, feel free to work with a reduced train and valid size for speed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In bert we need all inputs to have the same length, we will use the first 220 characters. \n",
    "MAX_SEQUENCE_LENGTH = 220\n",
    "SEED = 1234\n",
    "# We shall run a single epoch (ie. one pass over the data)\n",
    "EPOCHS = 1\n",
    "PATH = '.'\n",
    "#'/root/v2/week06/hw' # /root/v2/week06/hw\"\n",
    "DATA_DIR = os.path.join(PATH, \"data\")\n",
    "WORK_DIR = os.path.join(PATH, \"workingdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please use a small size here for experimentation purposes.  Once comfortable, we could increase it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and training sizes are here.\n",
    "# selected such that it won't take forever to run this \n",
    "train_size= 20000 # 1000000 \n",
    "valid_size= 10000  # 500000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the files you downloaded earlier when you ran `download.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall install pytorch BERT implementation.   \n",
    "If you would like to experiment with or view any code (purely optional, and not graded :) ), you can copy the files from the repo https://github.com/huggingface/pytorch-pretrained-BERT  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig, BertTokenizer, BertForSequenceClassification, BertTokenizerFast\n",
    "from transformers import AdamW as BertAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now load the model. When you run this, comment out the `capture` command to understand the archecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# bert_config = BertConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the BERT Fast tokenizer and convert the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "train_all = pd.read_csv(os.path.join(DATA_DIR, \"train.csv.zip\")).sample(train_size+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_all))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train_all['comment_text'] = train_all['comment_text'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### sequences = convert_lines(train_all[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "# sequences = tokenizer(train_all[\"comment_text\"].fillna(\"DUMMY_VALUE\").tolist(), truncation=True, add_special_tokens=True, padding=True, max_length = MAX_SEQUENCE_LENGTH)\n",
    "sequences = tokenizer(train_all[\"comment_text\"].fillna(\"DUMMY_VALUE\").tolist(), truncation=True, add_special_tokens=True, padding=True, max_length = MAX_SEQUENCE_LENGTH)\n",
    "train_all=train_all.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = sequences.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a binary problem, we change our target to [0,1], instead of float.   \n",
    "We also split the dataset into a training and validation set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all['target']=(train_all['target']>=0.5).astype(float)\n",
    "# Training data - sentences\n",
    "X = sequences[:train_size] \n",
    "# Target - the toxicity. \n",
    "y = train_all[['target']].values[:train_size]\n",
    "X_val = sequences[train_size:]                \n",
    "y_val = train_all[['target']].values[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=train_all.tail(valid_size).copy()\n",
    "train_df=train_all.head(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long),torch.tensor(y,dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns=['target']\n",
    "lr=2e-5\n",
    "batch_size = 32\n",
    "accumulation_steps=1\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=len(y_columns)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, please count the number of available GPUs. Then, if you have more than one, use torch.nn.DataParallel.  You also may want to change the batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fille in with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(WORK_DIR,cache_dir=None,num_labels=len(y_columns))\n",
    "model.zero_grad()\n",
    "model = model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "train = train_dataset\n",
    "\n",
    "num_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=lr)\n",
    "\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train your model by iterating through batches in a single epoch of the data. Note how long it takes**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import trange, tqdm, \n",
    "# from tqdm import tnrange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm, tqdm_notebook\n",
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model=model.train()\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# tq = tqdm(range(EPOCHS))\n",
    "# tq = tqdm.notebook(range(EPOCHS))\n",
    "\n",
    "tq = trange(EPOCHS)\n",
    "for epoch in tq:\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0.\n",
    "    lossf=None\n",
    "#    tk0 = tqdm_notebook.tqdm(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "    tk0 = tqdm(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n",
    "    for i,(x_batch, y_batch) in tk0:\n",
    "        if i % 100 == 0:\n",
    "            print(\"batch: \", i, \" of \", len(train_loader))\n",
    "        with autocast():\n",
    "            y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)[0]\n",
    "            loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "        scaler.scale(loss).backward()\n",
    "##        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)[0]\n",
    "##        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "##        loss.backward()\n",
    "        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "#            optimizer.step()                            # Now we can do an optimizer step\n",
    "#            optimizer.zero_grad()\n",
    "\n",
    "#        if n_gpu > 1:\n",
    "#            loss = loss.mean()\n",
    "        if lossf:\n",
    "            lossf = 0.98*lossf+0.02*loss.item()\n",
    "        else:\n",
    "            lossf = loss.item()\n",
    "\n",
    "        tk0.set_postfix(loss = lossf)\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "#    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now make a prediction for your validation set.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use DataParallel to run eval on more than GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_size = 2048\n",
    "valid_preds = np.zeros((len(X_val)))\n",
    "valid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "tk0 = tqdm_notebook(valid_loader)\n",
    "for i,(x_batch,)  in enumerate(tk0):\n",
    "    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)[0]\n",
    "    valid_preds[i*val_batch_size:(i+1)*val_batch_size]=pred[:,0].detach().cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds=torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "print('AUC score {:.5f}'.format(roc_auc_score(test_df['target'].values, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
