# Lecture 9: High Performance Computing and Distributed Model training

HPC, MPI, Distributed Training. An overview and a bit of history.   HPC vs HTC/Big Data.  Typical HPC problems.  Architecture of supercomputers. Interconnect topologies: fat tree, torus.  FLOPs, Top500.  Scaling: strong, weak, Amdahlâ€™s law.  Programming for HPC systems.  OpenMPI overview. Uber Horovod.  Distributed Deep Learning Model training. Applying distributed training: Generative Adversarial Networks (GANs).

## Reading:

* What is Open MPI? https://www.open-mpi.org/faq/?category=general
* Top 500 Supercomputers list: https://www.top500.org/lists/top500/
* IBM Summit: https://en.wikipedia.org/wiki/Summit_(supercomputer)
* Meet Uber Horovod: https://eng.uber.com/horovod/
* Progressive Growing of GANs: https://github.com/tkarras/progressive_growing_of_gans
* Nvidia GPU Direct: https://developer.nvidia.com/gpudirect


